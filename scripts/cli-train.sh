python ../fairseq/fairseq_cli/train.py data_bin.bak \
  --finetune-from-model  "m2m_model/418M_last_checkpoint.pt"\
  --save-dir checkpoint \
  --task translation_multi_simple_epoch \
  --encoder-normalize-before \
  --lang-pairs "en-yo" \
  --batch-size 1 \
  --decoder-normalize-before \
  --encoder-langtok src \
  --decoder-langtok \
  --criterion cross_entropy \
  --optimizer adafactor \
  --lr-scheduler cosine \
  --lr 3e-05 \
  --max-update 40000 \
  --update-freq 2 \
  --save-interval 1 \
  --save-interval-updates 5000 \
  --keep-interval-updates 10 \
  --no-epoch-checkpoints \
  --log-format simple \
  --log-interval 2 \
  --patience 10 \
  --arch transformer_wmt_en_de_big_split \
  --encoder-layers 12 --decoder-layers 12 \
  --share-decoder-input-output-embed \
  --ddp-backend no_c10d \
  --max-epoch 10 \
  --wandb-project "Yoruba M2M" \
  --split-embeddings 2